{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is being utilized\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../models\")\n",
    "\n",
    "from models import ResNetSimCLR\n",
    "import torch \n",
    "\n",
    "model = ResNetSimCLR(size=50, device=\"cuda\", embedding_dim=64)\n",
    "checkpoint = torch.load(\"../checkpoints/checkpoint_epoch_20.pt\", map_location=\"cuda\")\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "# Freeze all parameters in ResNetSimCLR\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.model.conv1.weight: Frozen\n",
      "encoder.model.bn1.weight: Frozen\n",
      "encoder.model.bn1.bias: Frozen\n",
      "encoder.model.layer1.0.conv1.weight: Frozen\n",
      "encoder.model.layer1.0.bn1.weight: Frozen\n",
      "encoder.model.layer1.0.bn1.bias: Frozen\n",
      "encoder.model.layer1.0.conv2.weight: Frozen\n",
      "encoder.model.layer1.0.bn2.weight: Frozen\n",
      "encoder.model.layer1.0.bn2.bias: Frozen\n",
      "encoder.model.layer1.0.conv3.weight: Frozen\n",
      "encoder.model.layer1.0.bn3.weight: Frozen\n",
      "encoder.model.layer1.0.bn3.bias: Frozen\n",
      "encoder.model.layer1.0.downsample.0.weight: Frozen\n",
      "encoder.model.layer1.0.downsample.1.weight: Frozen\n",
      "encoder.model.layer1.0.downsample.1.bias: Frozen\n",
      "encoder.model.layer1.1.conv1.weight: Frozen\n",
      "encoder.model.layer1.1.bn1.weight: Frozen\n",
      "encoder.model.layer1.1.bn1.bias: Frozen\n",
      "encoder.model.layer1.1.conv2.weight: Frozen\n",
      "encoder.model.layer1.1.bn2.weight: Frozen\n",
      "encoder.model.layer1.1.bn2.bias: Frozen\n",
      "encoder.model.layer1.1.conv3.weight: Frozen\n",
      "encoder.model.layer1.1.bn3.weight: Frozen\n",
      "encoder.model.layer1.1.bn3.bias: Frozen\n",
      "encoder.model.layer1.2.conv1.weight: Frozen\n",
      "encoder.model.layer1.2.bn1.weight: Frozen\n",
      "encoder.model.layer1.2.bn1.bias: Frozen\n",
      "encoder.model.layer1.2.conv2.weight: Frozen\n",
      "encoder.model.layer1.2.bn2.weight: Frozen\n",
      "encoder.model.layer1.2.bn2.bias: Frozen\n",
      "encoder.model.layer1.2.conv3.weight: Frozen\n",
      "encoder.model.layer1.2.bn3.weight: Frozen\n",
      "encoder.model.layer1.2.bn3.bias: Frozen\n",
      "encoder.model.layer2.0.conv1.weight: Frozen\n",
      "encoder.model.layer2.0.bn1.weight: Frozen\n",
      "encoder.model.layer2.0.bn1.bias: Frozen\n",
      "encoder.model.layer2.0.conv2.weight: Frozen\n",
      "encoder.model.layer2.0.bn2.weight: Frozen\n",
      "encoder.model.layer2.0.bn2.bias: Frozen\n",
      "encoder.model.layer2.0.conv3.weight: Frozen\n",
      "encoder.model.layer2.0.bn3.weight: Frozen\n",
      "encoder.model.layer2.0.bn3.bias: Frozen\n",
      "encoder.model.layer2.0.downsample.0.weight: Frozen\n",
      "encoder.model.layer2.0.downsample.1.weight: Frozen\n",
      "encoder.model.layer2.0.downsample.1.bias: Frozen\n",
      "encoder.model.layer2.1.conv1.weight: Frozen\n",
      "encoder.model.layer2.1.bn1.weight: Frozen\n",
      "encoder.model.layer2.1.bn1.bias: Frozen\n",
      "encoder.model.layer2.1.conv2.weight: Frozen\n",
      "encoder.model.layer2.1.bn2.weight: Frozen\n",
      "encoder.model.layer2.1.bn2.bias: Frozen\n",
      "encoder.model.layer2.1.conv3.weight: Frozen\n",
      "encoder.model.layer2.1.bn3.weight: Frozen\n",
      "encoder.model.layer2.1.bn3.bias: Frozen\n",
      "encoder.model.layer2.2.conv1.weight: Frozen\n",
      "encoder.model.layer2.2.bn1.weight: Frozen\n",
      "encoder.model.layer2.2.bn1.bias: Frozen\n",
      "encoder.model.layer2.2.conv2.weight: Frozen\n",
      "encoder.model.layer2.2.bn2.weight: Frozen\n",
      "encoder.model.layer2.2.bn2.bias: Frozen\n",
      "encoder.model.layer2.2.conv3.weight: Frozen\n",
      "encoder.model.layer2.2.bn3.weight: Frozen\n",
      "encoder.model.layer2.2.bn3.bias: Frozen\n",
      "encoder.model.layer2.3.conv1.weight: Frozen\n",
      "encoder.model.layer2.3.bn1.weight: Frozen\n",
      "encoder.model.layer2.3.bn1.bias: Frozen\n",
      "encoder.model.layer2.3.conv2.weight: Frozen\n",
      "encoder.model.layer2.3.bn2.weight: Frozen\n",
      "encoder.model.layer2.3.bn2.bias: Frozen\n",
      "encoder.model.layer2.3.conv3.weight: Frozen\n",
      "encoder.model.layer2.3.bn3.weight: Frozen\n",
      "encoder.model.layer2.3.bn3.bias: Frozen\n",
      "encoder.model.layer3.0.conv1.weight: Frozen\n",
      "encoder.model.layer3.0.bn1.weight: Frozen\n",
      "encoder.model.layer3.0.bn1.bias: Frozen\n",
      "encoder.model.layer3.0.conv2.weight: Frozen\n",
      "encoder.model.layer3.0.bn2.weight: Frozen\n",
      "encoder.model.layer3.0.bn2.bias: Frozen\n",
      "encoder.model.layer3.0.conv3.weight: Frozen\n",
      "encoder.model.layer3.0.bn3.weight: Frozen\n",
      "encoder.model.layer3.0.bn3.bias: Frozen\n",
      "encoder.model.layer3.0.downsample.0.weight: Frozen\n",
      "encoder.model.layer3.0.downsample.1.weight: Frozen\n",
      "encoder.model.layer3.0.downsample.1.bias: Frozen\n",
      "encoder.model.layer3.1.conv1.weight: Frozen\n",
      "encoder.model.layer3.1.bn1.weight: Frozen\n",
      "encoder.model.layer3.1.bn1.bias: Frozen\n",
      "encoder.model.layer3.1.conv2.weight: Frozen\n",
      "encoder.model.layer3.1.bn2.weight: Frozen\n",
      "encoder.model.layer3.1.bn2.bias: Frozen\n",
      "encoder.model.layer3.1.conv3.weight: Frozen\n",
      "encoder.model.layer3.1.bn3.weight: Frozen\n",
      "encoder.model.layer3.1.bn3.bias: Frozen\n",
      "encoder.model.layer3.2.conv1.weight: Frozen\n",
      "encoder.model.layer3.2.bn1.weight: Frozen\n",
      "encoder.model.layer3.2.bn1.bias: Frozen\n",
      "encoder.model.layer3.2.conv2.weight: Frozen\n",
      "encoder.model.layer3.2.bn2.weight: Frozen\n",
      "encoder.model.layer3.2.bn2.bias: Frozen\n",
      "encoder.model.layer3.2.conv3.weight: Frozen\n",
      "encoder.model.layer3.2.bn3.weight: Frozen\n",
      "encoder.model.layer3.2.bn3.bias: Frozen\n",
      "encoder.model.layer3.3.conv1.weight: Frozen\n",
      "encoder.model.layer3.3.bn1.weight: Frozen\n",
      "encoder.model.layer3.3.bn1.bias: Frozen\n",
      "encoder.model.layer3.3.conv2.weight: Frozen\n",
      "encoder.model.layer3.3.bn2.weight: Frozen\n",
      "encoder.model.layer3.3.bn2.bias: Frozen\n",
      "encoder.model.layer3.3.conv3.weight: Frozen\n",
      "encoder.model.layer3.3.bn3.weight: Frozen\n",
      "encoder.model.layer3.3.bn3.bias: Frozen\n",
      "encoder.model.layer3.4.conv1.weight: Frozen\n",
      "encoder.model.layer3.4.bn1.weight: Frozen\n",
      "encoder.model.layer3.4.bn1.bias: Frozen\n",
      "encoder.model.layer3.4.conv2.weight: Frozen\n",
      "encoder.model.layer3.4.bn2.weight: Frozen\n",
      "encoder.model.layer3.4.bn2.bias: Frozen\n",
      "encoder.model.layer3.4.conv3.weight: Frozen\n",
      "encoder.model.layer3.4.bn3.weight: Frozen\n",
      "encoder.model.layer3.4.bn3.bias: Frozen\n",
      "encoder.model.layer3.5.conv1.weight: Frozen\n",
      "encoder.model.layer3.5.bn1.weight: Frozen\n",
      "encoder.model.layer3.5.bn1.bias: Frozen\n",
      "encoder.model.layer3.5.conv2.weight: Frozen\n",
      "encoder.model.layer3.5.bn2.weight: Frozen\n",
      "encoder.model.layer3.5.bn2.bias: Frozen\n",
      "encoder.model.layer3.5.conv3.weight: Frozen\n",
      "encoder.model.layer3.5.bn3.weight: Frozen\n",
      "encoder.model.layer3.5.bn3.bias: Frozen\n",
      "encoder.model.layer4.0.conv1.weight: Frozen\n",
      "encoder.model.layer4.0.bn1.weight: Frozen\n",
      "encoder.model.layer4.0.bn1.bias: Frozen\n",
      "encoder.model.layer4.0.conv2.weight: Frozen\n",
      "encoder.model.layer4.0.bn2.weight: Frozen\n",
      "encoder.model.layer4.0.bn2.bias: Frozen\n",
      "encoder.model.layer4.0.conv3.weight: Frozen\n",
      "encoder.model.layer4.0.bn3.weight: Frozen\n",
      "encoder.model.layer4.0.bn3.bias: Frozen\n",
      "encoder.model.layer4.0.downsample.0.weight: Frozen\n",
      "encoder.model.layer4.0.downsample.1.weight: Frozen\n",
      "encoder.model.layer4.0.downsample.1.bias: Frozen\n",
      "encoder.model.layer4.1.conv1.weight: Frozen\n",
      "encoder.model.layer4.1.bn1.weight: Frozen\n",
      "encoder.model.layer4.1.bn1.bias: Frozen\n",
      "encoder.model.layer4.1.conv2.weight: Frozen\n",
      "encoder.model.layer4.1.bn2.weight: Frozen\n",
      "encoder.model.layer4.1.bn2.bias: Frozen\n",
      "encoder.model.layer4.1.conv3.weight: Frozen\n",
      "encoder.model.layer4.1.bn3.weight: Frozen\n",
      "encoder.model.layer4.1.bn3.bias: Frozen\n",
      "encoder.model.layer4.2.conv1.weight: Frozen\n",
      "encoder.model.layer4.2.bn1.weight: Frozen\n",
      "encoder.model.layer4.2.bn1.bias: Frozen\n",
      "encoder.model.layer4.2.conv2.weight: Frozen\n",
      "encoder.model.layer4.2.bn2.weight: Frozen\n",
      "encoder.model.layer4.2.bn2.bias: Frozen\n",
      "encoder.model.layer4.2.conv3.weight: Frozen\n",
      "encoder.model.layer4.2.bn3.weight: Frozen\n",
      "encoder.model.layer4.2.bn3.bias: Frozen\n",
      "encoder.model.fc.0.weight: Frozen\n",
      "encoder.model.fc.0.bias: Frozen\n",
      "encoder.model.fc.2.weight: Frozen\n",
      "encoder.model.fc.2.bias: Frozen\n",
      "fc1.weight: Trainable\n",
      "fc1.bias: Trainable\n",
      "fc2.weight: Trainable\n",
      "fc2.bias: Trainable\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        self.encoder = base_model  # Use the existing model as the encoder\n",
    "        # Add new layers on top of the encoder\n",
    "        self.fc1 = nn.Linear(base_model.embedding_dim, 64)  # First FC layer\n",
    "        self.relu = nn.ReLU()  # Activation function\n",
    "        self.fc2 = nn.Linear(64, 15)  # Classification head for 10 classes\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax for probabilities\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the encoder\n",
    "        x = self.encoder(x)\n",
    "        # Pass through the new fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)  # Apply softmax for probabilities\n",
    "        return x\n",
    "\n",
    "# Instantiate the custom model\n",
    "custom_model = ClassifierHead(model)\n",
    "\n",
    "# Verify trainable parameters\n",
    "for name, param in custom_model.named_parameters():\n",
    "    print(f\"{name}: {'Trainable' if param.requires_grad else 'Frozen'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Suitable for logits or probabilities\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, custom_model.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters being optimized:\n",
      "Parameter group 0:\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([15, 64])\n",
      "torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameters being optimized:\")\n",
    "for i, param_group in enumerate(optimizer.param_groups):\n",
    "    print(f\"Parameter group {i}:\")\n",
    "    for param in param_group[\"params\"]:\n",
    "        print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LDRMMM01\\Desktop\\contrastive_learning\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class MvTecDataset(Dataset):\n",
    "    def __init__(self, json_path='./dataset/dataset.json', train=True, img_size=256,\n",
    "                 mean=[0.4914, 0.4822, 0.4465],  std=[0.2470, 0.2435, 0.2616]):\n",
    "        \"\"\"Loads image paths and labels from a JSON file.\"\"\"\n",
    "        with open(json_path, 'r') as file:\n",
    "            dataset_dict = json.load(file)\n",
    "        \n",
    "        self.items = dataset_dict['train'] if train else dataset_dict['val']\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip() if train else transforms.RandomApply([]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.items[idx]\n",
    "        img_path, class_label = item[\"path\"], item[\"label\"]\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image: {img_path} - {e}\")\n",
    "            return None  # Skip problematic images\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        return img, class_label\n",
    "\n",
    "def create_dataloaders(json_path='./dataset/dataset.json', batch_size=32, img_size=256, num_workers=4):\n",
    "    \"\"\"Creates DataLoaders for training and validation datasets.\"\"\"\n",
    "    train_dataset = MvTecDataset(json_path=json_path, train=True, img_size=img_size)\n",
    "    val_dataset = MvTecDataset(json_path=json_path, train=False, img_size=img_size)\n",
    "    \n",
    "    # Splitting validation dataset: 70% - 30%\n",
    "    val_size = int(0.7 * len(val_dataset))\n",
    "    test_size = len(val_dataset) - val_size\n",
    "    val_subset, test_subset = random_split(val_dataset, [val_size, test_size])\n",
    "\n",
    "    # Creating DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Debugging Tip: If you get worker errors, try setting num_workers=0\n",
    "temp, data_loader_train, data_loader_val = create_dataloaders(num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 2.7070 | Train Acc: 19.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Val Loss: 2.7055 | Val Acc: 25.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Train Loss: 2.7033 | Train Acc: 32.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Validation: 100%|██████████| 9/9 [00:06<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Val Loss: 2.6998 | Val Acc: 39.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Train Loss: 2.6972 | Train Acc: 34.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Val Loss: 2.6898 | Val Acc: 39.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Train Loss: 2.6856 | Train Acc: 29.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Validation: 100%|██████████| 9/9 [00:06<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Val Loss: 2.6692 | Val Acc: 34.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Train Loss: 2.6619 | Train Acc: 26.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Val Loss: 2.6277 | Val Acc: 29.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Train Loss: 2.6224 | Train Acc: 29.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Val Loss: 2.5760 | Val Acc: 37.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Train Loss: 2.5808 | Train Acc: 35.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Val Loss: 2.5291 | Val Acc: 50.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Training: 100%|██████████| 20/20 [00:17<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Train Loss: 2.5395 | Train Acc: 47.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Val Loss: 2.4799 | Val Acc: 54.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Train Loss: 2.4948 | Train Acc: 51.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Val Loss: 2.4287 | Val Acc: 56.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Training: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Train Loss: 2.4490 | Train Acc: 52.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Val Loss: 2.3801 | Val Acc: 56.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Training: 100%|██████████| 20/20 [00:17<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Train Loss: 2.4060 | Train Acc: 53.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Val Loss: 2.3392 | Val Acc: 56.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Train Loss: 2.3707 | Train Acc: 53.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Val Loss: 2.3088 | Val Acc: 56.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Train Loss: 2.3438 | Train Acc: 53.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Validation: 100%|██████████| 9/9 [00:06<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Val Loss: 2.2862 | Val Acc: 56.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Train Loss: 2.3221 | Train Acc: 53.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Val Loss: 2.2678 | Val Acc: 56.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Train Loss: 2.3024 | Train Acc: 53.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Val Loss: 2.2507 | Val Acc: 56.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Train Loss: 2.2817 | Train Acc: 55.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Val Loss: 2.2326 | Val Acc: 62.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Train Loss: 2.2581 | Train Acc: 68.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Validation: 100%|██████████| 9/9 [00:06<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Val Loss: 2.2134 | Val Acc: 71.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Train Loss: 2.2326 | Train Acc: 73.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Validation: 100%|██████████| 9/9 [00:06<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Val Loss: 2.1938 | Val Acc: 72.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Train Loss: 2.2066 | Train Acc: 75.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Val Loss: 2.1737 | Val Acc: 75.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Train Loss: 2.1801 | Train Acc: 78.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Val Loss: 2.1534 | Val Acc: 76.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, custom_model.parameters()), lr=learning_rate)\n",
    "\n",
    "# Move model to device\n",
    "custom_model.to(device)\n",
    "\n",
    "# Training loop with validation\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    custom_model.train()\n",
    "    train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(tqdm(data_loader_train, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Zero the gradient buffers\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = custom_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track metrics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_train += targets.size(0)\n",
    "        correct_train += predicted.eq(targets).sum().item()\n",
    "\n",
    "    train_acc = 100. * correct_train / total_train\n",
    "    avg_train_loss = train_loss / len(data_loader_train)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "    # Validation phase\n",
    "    custom_model.eval()\n",
    "    val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(data_loader_val, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\")):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = custom_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Track metrics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_val += targets.size(0)\n",
    "            correct_val += predicted.eq(targets).sum().item()\n",
    "\n",
    "    val_acc = 100. * correct_val / total_val\n",
    "    avg_val_loss = val_loss / len(data_loader_val)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters:\n",
      "encoder.conv1.weight: Frozen\n",
      "encoder.bn1.weight: Frozen\n",
      "encoder.bn1.bias: Frozen\n",
      "encoder.layer1.0.conv1.weight: Frozen\n",
      "encoder.layer1.0.bn1.weight: Frozen\n",
      "encoder.layer1.0.bn1.bias: Frozen\n",
      "encoder.layer1.0.conv2.weight: Frozen\n",
      "encoder.layer1.0.bn2.weight: Frozen\n",
      "encoder.layer1.0.bn2.bias: Frozen\n",
      "encoder.layer1.0.conv3.weight: Frozen\n",
      "encoder.layer1.0.bn3.weight: Frozen\n",
      "encoder.layer1.0.bn3.bias: Frozen\n",
      "encoder.layer1.0.downsample.0.weight: Frozen\n",
      "encoder.layer1.0.downsample.1.weight: Frozen\n",
      "encoder.layer1.0.downsample.1.bias: Frozen\n",
      "encoder.layer1.1.conv1.weight: Frozen\n",
      "encoder.layer1.1.bn1.weight: Frozen\n",
      "encoder.layer1.1.bn1.bias: Frozen\n",
      "encoder.layer1.1.conv2.weight: Frozen\n",
      "encoder.layer1.1.bn2.weight: Frozen\n",
      "encoder.layer1.1.bn2.bias: Frozen\n",
      "encoder.layer1.1.conv3.weight: Frozen\n",
      "encoder.layer1.1.bn3.weight: Frozen\n",
      "encoder.layer1.1.bn3.bias: Frozen\n",
      "encoder.layer1.2.conv1.weight: Frozen\n",
      "encoder.layer1.2.bn1.weight: Frozen\n",
      "encoder.layer1.2.bn1.bias: Frozen\n",
      "encoder.layer1.2.conv2.weight: Frozen\n",
      "encoder.layer1.2.bn2.weight: Frozen\n",
      "encoder.layer1.2.bn2.bias: Frozen\n",
      "encoder.layer1.2.conv3.weight: Frozen\n",
      "encoder.layer1.2.bn3.weight: Frozen\n",
      "encoder.layer1.2.bn3.bias: Frozen\n",
      "encoder.layer2.0.conv1.weight: Frozen\n",
      "encoder.layer2.0.bn1.weight: Frozen\n",
      "encoder.layer2.0.bn1.bias: Frozen\n",
      "encoder.layer2.0.conv2.weight: Frozen\n",
      "encoder.layer2.0.bn2.weight: Frozen\n",
      "encoder.layer2.0.bn2.bias: Frozen\n",
      "encoder.layer2.0.conv3.weight: Frozen\n",
      "encoder.layer2.0.bn3.weight: Frozen\n",
      "encoder.layer2.0.bn3.bias: Frozen\n",
      "encoder.layer2.0.downsample.0.weight: Frozen\n",
      "encoder.layer2.0.downsample.1.weight: Frozen\n",
      "encoder.layer2.0.downsample.1.bias: Frozen\n",
      "encoder.layer2.1.conv1.weight: Frozen\n",
      "encoder.layer2.1.bn1.weight: Frozen\n",
      "encoder.layer2.1.bn1.bias: Frozen\n",
      "encoder.layer2.1.conv2.weight: Frozen\n",
      "encoder.layer2.1.bn2.weight: Frozen\n",
      "encoder.layer2.1.bn2.bias: Frozen\n",
      "encoder.layer2.1.conv3.weight: Frozen\n",
      "encoder.layer2.1.bn3.weight: Frozen\n",
      "encoder.layer2.1.bn3.bias: Frozen\n",
      "encoder.layer2.2.conv1.weight: Frozen\n",
      "encoder.layer2.2.bn1.weight: Frozen\n",
      "encoder.layer2.2.bn1.bias: Frozen\n",
      "encoder.layer2.2.conv2.weight: Frozen\n",
      "encoder.layer2.2.bn2.weight: Frozen\n",
      "encoder.layer2.2.bn2.bias: Frozen\n",
      "encoder.layer2.2.conv3.weight: Frozen\n",
      "encoder.layer2.2.bn3.weight: Frozen\n",
      "encoder.layer2.2.bn3.bias: Frozen\n",
      "encoder.layer2.3.conv1.weight: Frozen\n",
      "encoder.layer2.3.bn1.weight: Frozen\n",
      "encoder.layer2.3.bn1.bias: Frozen\n",
      "encoder.layer2.3.conv2.weight: Frozen\n",
      "encoder.layer2.3.bn2.weight: Frozen\n",
      "encoder.layer2.3.bn2.bias: Frozen\n",
      "encoder.layer2.3.conv3.weight: Frozen\n",
      "encoder.layer2.3.bn3.weight: Frozen\n",
      "encoder.layer2.3.bn3.bias: Frozen\n",
      "encoder.layer3.0.conv1.weight: Frozen\n",
      "encoder.layer3.0.bn1.weight: Frozen\n",
      "encoder.layer3.0.bn1.bias: Frozen\n",
      "encoder.layer3.0.conv2.weight: Frozen\n",
      "encoder.layer3.0.bn2.weight: Frozen\n",
      "encoder.layer3.0.bn2.bias: Frozen\n",
      "encoder.layer3.0.conv3.weight: Frozen\n",
      "encoder.layer3.0.bn3.weight: Frozen\n",
      "encoder.layer3.0.bn3.bias: Frozen\n",
      "encoder.layer3.0.downsample.0.weight: Frozen\n",
      "encoder.layer3.0.downsample.1.weight: Frozen\n",
      "encoder.layer3.0.downsample.1.bias: Frozen\n",
      "encoder.layer3.1.conv1.weight: Frozen\n",
      "encoder.layer3.1.bn1.weight: Frozen\n",
      "encoder.layer3.1.bn1.bias: Frozen\n",
      "encoder.layer3.1.conv2.weight: Frozen\n",
      "encoder.layer3.1.bn2.weight: Frozen\n",
      "encoder.layer3.1.bn2.bias: Frozen\n",
      "encoder.layer3.1.conv3.weight: Frozen\n",
      "encoder.layer3.1.bn3.weight: Frozen\n",
      "encoder.layer3.1.bn3.bias: Frozen\n",
      "encoder.layer3.2.conv1.weight: Frozen\n",
      "encoder.layer3.2.bn1.weight: Frozen\n",
      "encoder.layer3.2.bn1.bias: Frozen\n",
      "encoder.layer3.2.conv2.weight: Frozen\n",
      "encoder.layer3.2.bn2.weight: Frozen\n",
      "encoder.layer3.2.bn2.bias: Frozen\n",
      "encoder.layer3.2.conv3.weight: Frozen\n",
      "encoder.layer3.2.bn3.weight: Frozen\n",
      "encoder.layer3.2.bn3.bias: Frozen\n",
      "encoder.layer3.3.conv1.weight: Frozen\n",
      "encoder.layer3.3.bn1.weight: Frozen\n",
      "encoder.layer3.3.bn1.bias: Frozen\n",
      "encoder.layer3.3.conv2.weight: Frozen\n",
      "encoder.layer3.3.bn2.weight: Frozen\n",
      "encoder.layer3.3.bn2.bias: Frozen\n",
      "encoder.layer3.3.conv3.weight: Frozen\n",
      "encoder.layer3.3.bn3.weight: Frozen\n",
      "encoder.layer3.3.bn3.bias: Frozen\n",
      "encoder.layer3.4.conv1.weight: Frozen\n",
      "encoder.layer3.4.bn1.weight: Frozen\n",
      "encoder.layer3.4.bn1.bias: Frozen\n",
      "encoder.layer3.4.conv2.weight: Frozen\n",
      "encoder.layer3.4.bn2.weight: Frozen\n",
      "encoder.layer3.4.bn2.bias: Frozen\n",
      "encoder.layer3.4.conv3.weight: Frozen\n",
      "encoder.layer3.4.bn3.weight: Frozen\n",
      "encoder.layer3.4.bn3.bias: Frozen\n",
      "encoder.layer3.5.conv1.weight: Frozen\n",
      "encoder.layer3.5.bn1.weight: Frozen\n",
      "encoder.layer3.5.bn1.bias: Frozen\n",
      "encoder.layer3.5.conv2.weight: Frozen\n",
      "encoder.layer3.5.bn2.weight: Frozen\n",
      "encoder.layer3.5.bn2.bias: Frozen\n",
      "encoder.layer3.5.conv3.weight: Frozen\n",
      "encoder.layer3.5.bn3.weight: Frozen\n",
      "encoder.layer3.5.bn3.bias: Frozen\n",
      "encoder.layer4.0.conv1.weight: Frozen\n",
      "encoder.layer4.0.bn1.weight: Frozen\n",
      "encoder.layer4.0.bn1.bias: Frozen\n",
      "encoder.layer4.0.conv2.weight: Frozen\n",
      "encoder.layer4.0.bn2.weight: Frozen\n",
      "encoder.layer4.0.bn2.bias: Frozen\n",
      "encoder.layer4.0.conv3.weight: Frozen\n",
      "encoder.layer4.0.bn3.weight: Frozen\n",
      "encoder.layer4.0.bn3.bias: Frozen\n",
      "encoder.layer4.0.downsample.0.weight: Frozen\n",
      "encoder.layer4.0.downsample.1.weight: Frozen\n",
      "encoder.layer4.0.downsample.1.bias: Frozen\n",
      "encoder.layer4.1.conv1.weight: Frozen\n",
      "encoder.layer4.1.bn1.weight: Frozen\n",
      "encoder.layer4.1.bn1.bias: Frozen\n",
      "encoder.layer4.1.conv2.weight: Frozen\n",
      "encoder.layer4.1.bn2.weight: Frozen\n",
      "encoder.layer4.1.bn2.bias: Frozen\n",
      "encoder.layer4.1.conv3.weight: Frozen\n",
      "encoder.layer4.1.bn3.weight: Frozen\n",
      "encoder.layer4.1.bn3.bias: Frozen\n",
      "encoder.layer4.2.conv1.weight: Frozen\n",
      "encoder.layer4.2.bn1.weight: Frozen\n",
      "encoder.layer4.2.bn1.bias: Frozen\n",
      "encoder.layer4.2.conv2.weight: Frozen\n",
      "encoder.layer4.2.bn2.weight: Frozen\n",
      "encoder.layer4.2.bn2.bias: Frozen\n",
      "encoder.layer4.2.conv3.weight: Frozen\n",
      "encoder.layer4.2.bn3.weight: Frozen\n",
      "encoder.layer4.2.bn3.bias: Frozen\n",
      "fc1.weight: Trainable\n",
      "fc1.bias: Trainable\n",
      "fc2.weight: Trainable\n",
      "fc2.bias: Trainable\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# Define the ClassifierHead\n",
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self, base_model, embedding_dim):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        self.encoder = base_model  # Use the base model as the encoder\n",
    "        self.fc1 = nn.Linear(embedding_dim, 64)  # First FC layer\n",
    "        self.relu = nn.ReLU()  # Activation function\n",
    "        self.fc2 = nn.Linear(64, 15)  # Classification head for 10 classes\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax for probabilities\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the encoder\n",
    "        x = self.encoder(x)\n",
    "        # Pass through the new fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)  # Apply softmax for probabilities\n",
    "        return x\n",
    "\n",
    "# Initialize a randomly initialized ResNet\n",
    "random_resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Replace the final fully connected layer with an identity function\n",
    "random_resnet.fc = nn.Identity()\n",
    "\n",
    "# Freeze all parameters in the ResNet\n",
    "for param in random_resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define the embedding dimension (ResNet-50's output features are 2048)\n",
    "embedding_dim = 2048\n",
    "\n",
    "# Instantiate the custom model\n",
    "custom_model = ClassifierHead(random_resnet, embedding_dim)\n",
    "\n",
    "# Verify trainable parameters\n",
    "print(\"Trainable Parameters:\")\n",
    "for name, param in custom_model.named_parameters():\n",
    "    print(f\"{name}: {'Trainable' if param.requires_grad else 'Frozen'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\eng_apps\\Anaconda3\\envs\\simclr\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\eng_apps\\Anaconda3\\envs\\simclr\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s, accuracy=13.2, loss=2.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 2.6585, Accuracy: 13.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Validation: 100%|██████████| 9/9 [00:06<00:00,  1.29it/s, accuracy=12.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 12.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s, accuracy=21.1, loss=2.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Loss: 2.6126, Accuracy: 21.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.28it/s, accuracy=23.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 23.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s, accuracy=25.4, loss=2.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Loss: 2.5863, Accuracy: 25.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Validation: 100%|██████████| 9/9 [00:06<00:00,  1.29it/s, accuracy=30.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 30.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s, accuracy=27.6, loss=2.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Loss: 2.5670, Accuracy: 27.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.28it/s, accuracy=38.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 38.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s, accuracy=28, loss=2.55]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss: 2.5505, Accuracy: 28.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.26it/s, accuracy=36.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 36.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s, accuracy=30.9, loss=2.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Loss: 2.5403, Accuracy: 30.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.28it/s, accuracy=38.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 38.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s, accuracy=32.6, loss=2.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Loss: 2.5300, Accuracy: 32.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.28it/s, accuracy=40.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 40.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s, accuracy=34.2, loss=2.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Loss: 2.5129, Accuracy: 34.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Validation: 100%|██████████| 9/9 [00:06<00:00,  1.29it/s, accuracy=41]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 41.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s, accuracy=34.6, loss=2.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Loss: 2.4999, Accuracy: 34.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Validation: 100%|██████████| 9/9 [00:06<00:00,  1.29it/s, accuracy=41]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 41.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s, accuracy=35.7, loss=2.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Loss: 2.4864, Accuracy: 35.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.28it/s, accuracy=40.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 40.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s, accuracy=36.1, loss=2.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Loss: 2.4760, Accuracy: 36.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.28it/s, accuracy=40.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 40.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s, accuracy=37.2, loss=2.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Loss: 2.4667, Accuracy: 37.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Validation: 100%|██████████| 9/9 [00:06<00:00,  1.29it/s, accuracy=41]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 41.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s, accuracy=37.8, loss=2.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Loss: 2.4590, Accuracy: 37.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.28it/s, accuracy=41]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 41.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s, accuracy=38.6, loss=2.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Loss: 2.4524, Accuracy: 38.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Validation: 100%|██████████| 9/9 [00:06<00:00,  1.29it/s, accuracy=42.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 42.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s, accuracy=38.6, loss=2.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Loss: 2.4467, Accuracy: 38.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.28it/s, accuracy=43.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 43.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s, accuracy=38.7, loss=2.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Loss: 2.4423, Accuracy: 38.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Validation: 100%|██████████| 9/9 [00:06<00:00,  1.29it/s, accuracy=44.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 44.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s, accuracy=38.9, loss=2.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Loss: 2.4386, Accuracy: 38.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.28it/s, accuracy=45.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 45.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s, accuracy=39.1, loss=2.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Loss: 2.4351, Accuracy: 39.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.23it/s, accuracy=45.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 45.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Training: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s, accuracy=38.9, loss=2.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Loss: 2.4326, Accuracy: 38.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.28it/s, accuracy=47.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 47.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Training: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s, accuracy=39.5, loss=2.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Loss: 2.4308, Accuracy: 39.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Validation: 100%|██████████| 9/9 [00:07<00:00,  1.29it/s, accuracy=47.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 47.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "from tqdm import tqdm\n",
    "\n",
    "random_resnet = resnet50(pretrained=False)\n",
    "random_resnet.fc = nn.Identity()  # Replace the final layer with identity\n",
    "for param in random_resnet.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers\n",
    "\n",
    "embedding_dim = 2048\n",
    "custom_model = ClassifierHead(random_resnet, embedding_dim)\n",
    "\n",
    "# Move the model to GPU\n",
    "custom_model = custom_model.cuda()\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, custom_model.parameters()), lr=1e-3)\n",
    "\n",
    "epochs = 20  # Number of epochs\n",
    "for epoch in range(epochs):\n",
    "    custom_model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Wrap the batch iteration with tqdm for progress bar\n",
    "    with tqdm(data_loader_train, desc=f\"Epoch {epoch+1}/{epochs} - Training\") as pbar_train:\n",
    "        for inputs, labels in pbar_train:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()  # Move inputs and labels to GPU\n",
    "\n",
    "            optimizer.zero_grad()  # Clear gradients from previous step\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = custom_model(inputs)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()  # Backpropagate\n",
    "            \n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar_train.set_postfix(loss=running_loss / (pbar_train.n + 1), accuracy=100 * correct / total)\n",
    "\n",
    "    avg_loss = running_loss / len(data_loader_train)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    # Validation loop with tqdm\n",
    "    custom_model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        with tqdm(data_loader_val, desc=f\"Epoch {epoch+1}/{epochs} - Validation\") as pbar_val:\n",
    "            for inputs, labels in pbar_val:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()  # Move inputs and labels to GPU\n",
    "                \n",
    "                outputs = custom_model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                # Update progress bar\n",
    "                pbar_val.set_postfix(accuracy=100 * correct / total)\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simclr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
