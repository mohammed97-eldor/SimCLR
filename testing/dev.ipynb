{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is being utilized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3002/1677408530.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"../checkpoints/checkpoint_epoch_80.pt\", map_location=\"cuda\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../models\")\n",
    "\n",
    "from models import ResNetSimCLR\n",
    "import torch \n",
    "\n",
    "model = ResNetSimCLR(size=50, device=\"cuda\", embedding_dim=64)\n",
    "checkpoint = torch.load(\"../checkpoints/checkpoint_epoch_80.pt\", map_location=\"cuda\")\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "# Freeze all parameters in ResNetSimCLR\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.model.conv1.weight: Frozen\n",
      "encoder.model.bn1.weight: Frozen\n",
      "encoder.model.bn1.bias: Frozen\n",
      "encoder.model.layer1.0.conv1.weight: Frozen\n",
      "encoder.model.layer1.0.bn1.weight: Frozen\n",
      "encoder.model.layer1.0.bn1.bias: Frozen\n",
      "encoder.model.layer1.0.conv2.weight: Frozen\n",
      "encoder.model.layer1.0.bn2.weight: Frozen\n",
      "encoder.model.layer1.0.bn2.bias: Frozen\n",
      "encoder.model.layer1.0.conv3.weight: Frozen\n",
      "encoder.model.layer1.0.bn3.weight: Frozen\n",
      "encoder.model.layer1.0.bn3.bias: Frozen\n",
      "encoder.model.layer1.0.downsample.0.weight: Frozen\n",
      "encoder.model.layer1.0.downsample.1.weight: Frozen\n",
      "encoder.model.layer1.0.downsample.1.bias: Frozen\n",
      "encoder.model.layer1.1.conv1.weight: Frozen\n",
      "encoder.model.layer1.1.bn1.weight: Frozen\n",
      "encoder.model.layer1.1.bn1.bias: Frozen\n",
      "encoder.model.layer1.1.conv2.weight: Frozen\n",
      "encoder.model.layer1.1.bn2.weight: Frozen\n",
      "encoder.model.layer1.1.bn2.bias: Frozen\n",
      "encoder.model.layer1.1.conv3.weight: Frozen\n",
      "encoder.model.layer1.1.bn3.weight: Frozen\n",
      "encoder.model.layer1.1.bn3.bias: Frozen\n",
      "encoder.model.layer1.2.conv1.weight: Frozen\n",
      "encoder.model.layer1.2.bn1.weight: Frozen\n",
      "encoder.model.layer1.2.bn1.bias: Frozen\n",
      "encoder.model.layer1.2.conv2.weight: Frozen\n",
      "encoder.model.layer1.2.bn2.weight: Frozen\n",
      "encoder.model.layer1.2.bn2.bias: Frozen\n",
      "encoder.model.layer1.2.conv3.weight: Frozen\n",
      "encoder.model.layer1.2.bn3.weight: Frozen\n",
      "encoder.model.layer1.2.bn3.bias: Frozen\n",
      "encoder.model.layer2.0.conv1.weight: Frozen\n",
      "encoder.model.layer2.0.bn1.weight: Frozen\n",
      "encoder.model.layer2.0.bn1.bias: Frozen\n",
      "encoder.model.layer2.0.conv2.weight: Frozen\n",
      "encoder.model.layer2.0.bn2.weight: Frozen\n",
      "encoder.model.layer2.0.bn2.bias: Frozen\n",
      "encoder.model.layer2.0.conv3.weight: Frozen\n",
      "encoder.model.layer2.0.bn3.weight: Frozen\n",
      "encoder.model.layer2.0.bn3.bias: Frozen\n",
      "encoder.model.layer2.0.downsample.0.weight: Frozen\n",
      "encoder.model.layer2.0.downsample.1.weight: Frozen\n",
      "encoder.model.layer2.0.downsample.1.bias: Frozen\n",
      "encoder.model.layer2.1.conv1.weight: Frozen\n",
      "encoder.model.layer2.1.bn1.weight: Frozen\n",
      "encoder.model.layer2.1.bn1.bias: Frozen\n",
      "encoder.model.layer2.1.conv2.weight: Frozen\n",
      "encoder.model.layer2.1.bn2.weight: Frozen\n",
      "encoder.model.layer2.1.bn2.bias: Frozen\n",
      "encoder.model.layer2.1.conv3.weight: Frozen\n",
      "encoder.model.layer2.1.bn3.weight: Frozen\n",
      "encoder.model.layer2.1.bn3.bias: Frozen\n",
      "encoder.model.layer2.2.conv1.weight: Frozen\n",
      "encoder.model.layer2.2.bn1.weight: Frozen\n",
      "encoder.model.layer2.2.bn1.bias: Frozen\n",
      "encoder.model.layer2.2.conv2.weight: Frozen\n",
      "encoder.model.layer2.2.bn2.weight: Frozen\n",
      "encoder.model.layer2.2.bn2.bias: Frozen\n",
      "encoder.model.layer2.2.conv3.weight: Frozen\n",
      "encoder.model.layer2.2.bn3.weight: Frozen\n",
      "encoder.model.layer2.2.bn3.bias: Frozen\n",
      "encoder.model.layer2.3.conv1.weight: Frozen\n",
      "encoder.model.layer2.3.bn1.weight: Frozen\n",
      "encoder.model.layer2.3.bn1.bias: Frozen\n",
      "encoder.model.layer2.3.conv2.weight: Frozen\n",
      "encoder.model.layer2.3.bn2.weight: Frozen\n",
      "encoder.model.layer2.3.bn2.bias: Frozen\n",
      "encoder.model.layer2.3.conv3.weight: Frozen\n",
      "encoder.model.layer2.3.bn3.weight: Frozen\n",
      "encoder.model.layer2.3.bn3.bias: Frozen\n",
      "encoder.model.layer3.0.conv1.weight: Frozen\n",
      "encoder.model.layer3.0.bn1.weight: Frozen\n",
      "encoder.model.layer3.0.bn1.bias: Frozen\n",
      "encoder.model.layer3.0.conv2.weight: Frozen\n",
      "encoder.model.layer3.0.bn2.weight: Frozen\n",
      "encoder.model.layer3.0.bn2.bias: Frozen\n",
      "encoder.model.layer3.0.conv3.weight: Frozen\n",
      "encoder.model.layer3.0.bn3.weight: Frozen\n",
      "encoder.model.layer3.0.bn3.bias: Frozen\n",
      "encoder.model.layer3.0.downsample.0.weight: Frozen\n",
      "encoder.model.layer3.0.downsample.1.weight: Frozen\n",
      "encoder.model.layer3.0.downsample.1.bias: Frozen\n",
      "encoder.model.layer3.1.conv1.weight: Frozen\n",
      "encoder.model.layer3.1.bn1.weight: Frozen\n",
      "encoder.model.layer3.1.bn1.bias: Frozen\n",
      "encoder.model.layer3.1.conv2.weight: Frozen\n",
      "encoder.model.layer3.1.bn2.weight: Frozen\n",
      "encoder.model.layer3.1.bn2.bias: Frozen\n",
      "encoder.model.layer3.1.conv3.weight: Frozen\n",
      "encoder.model.layer3.1.bn3.weight: Frozen\n",
      "encoder.model.layer3.1.bn3.bias: Frozen\n",
      "encoder.model.layer3.2.conv1.weight: Frozen\n",
      "encoder.model.layer3.2.bn1.weight: Frozen\n",
      "encoder.model.layer3.2.bn1.bias: Frozen\n",
      "encoder.model.layer3.2.conv2.weight: Frozen\n",
      "encoder.model.layer3.2.bn2.weight: Frozen\n",
      "encoder.model.layer3.2.bn2.bias: Frozen\n",
      "encoder.model.layer3.2.conv3.weight: Frozen\n",
      "encoder.model.layer3.2.bn3.weight: Frozen\n",
      "encoder.model.layer3.2.bn3.bias: Frozen\n",
      "encoder.model.layer3.3.conv1.weight: Frozen\n",
      "encoder.model.layer3.3.bn1.weight: Frozen\n",
      "encoder.model.layer3.3.bn1.bias: Frozen\n",
      "encoder.model.layer3.3.conv2.weight: Frozen\n",
      "encoder.model.layer3.3.bn2.weight: Frozen\n",
      "encoder.model.layer3.3.bn2.bias: Frozen\n",
      "encoder.model.layer3.3.conv3.weight: Frozen\n",
      "encoder.model.layer3.3.bn3.weight: Frozen\n",
      "encoder.model.layer3.3.bn3.bias: Frozen\n",
      "encoder.model.layer3.4.conv1.weight: Frozen\n",
      "encoder.model.layer3.4.bn1.weight: Frozen\n",
      "encoder.model.layer3.4.bn1.bias: Frozen\n",
      "encoder.model.layer3.4.conv2.weight: Frozen\n",
      "encoder.model.layer3.4.bn2.weight: Frozen\n",
      "encoder.model.layer3.4.bn2.bias: Frozen\n",
      "encoder.model.layer3.4.conv3.weight: Frozen\n",
      "encoder.model.layer3.4.bn3.weight: Frozen\n",
      "encoder.model.layer3.4.bn3.bias: Frozen\n",
      "encoder.model.layer3.5.conv1.weight: Frozen\n",
      "encoder.model.layer3.5.bn1.weight: Frozen\n",
      "encoder.model.layer3.5.bn1.bias: Frozen\n",
      "encoder.model.layer3.5.conv2.weight: Frozen\n",
      "encoder.model.layer3.5.bn2.weight: Frozen\n",
      "encoder.model.layer3.5.bn2.bias: Frozen\n",
      "encoder.model.layer3.5.conv3.weight: Frozen\n",
      "encoder.model.layer3.5.bn3.weight: Frozen\n",
      "encoder.model.layer3.5.bn3.bias: Frozen\n",
      "encoder.model.layer4.0.conv1.weight: Frozen\n",
      "encoder.model.layer4.0.bn1.weight: Frozen\n",
      "encoder.model.layer4.0.bn1.bias: Frozen\n",
      "encoder.model.layer4.0.conv2.weight: Frozen\n",
      "encoder.model.layer4.0.bn2.weight: Frozen\n",
      "encoder.model.layer4.0.bn2.bias: Frozen\n",
      "encoder.model.layer4.0.conv3.weight: Frozen\n",
      "encoder.model.layer4.0.bn3.weight: Frozen\n",
      "encoder.model.layer4.0.bn3.bias: Frozen\n",
      "encoder.model.layer4.0.downsample.0.weight: Frozen\n",
      "encoder.model.layer4.0.downsample.1.weight: Frozen\n",
      "encoder.model.layer4.0.downsample.1.bias: Frozen\n",
      "encoder.model.layer4.1.conv1.weight: Frozen\n",
      "encoder.model.layer4.1.bn1.weight: Frozen\n",
      "encoder.model.layer4.1.bn1.bias: Frozen\n",
      "encoder.model.layer4.1.conv2.weight: Frozen\n",
      "encoder.model.layer4.1.bn2.weight: Frozen\n",
      "encoder.model.layer4.1.bn2.bias: Frozen\n",
      "encoder.model.layer4.1.conv3.weight: Frozen\n",
      "encoder.model.layer4.1.bn3.weight: Frozen\n",
      "encoder.model.layer4.1.bn3.bias: Frozen\n",
      "encoder.model.layer4.2.conv1.weight: Frozen\n",
      "encoder.model.layer4.2.bn1.weight: Frozen\n",
      "encoder.model.layer4.2.bn1.bias: Frozen\n",
      "encoder.model.layer4.2.conv2.weight: Frozen\n",
      "encoder.model.layer4.2.bn2.weight: Frozen\n",
      "encoder.model.layer4.2.bn2.bias: Frozen\n",
      "encoder.model.layer4.2.conv3.weight: Frozen\n",
      "encoder.model.layer4.2.bn3.weight: Frozen\n",
      "encoder.model.layer4.2.bn3.bias: Frozen\n",
      "encoder.model.fc.0.weight: Frozen\n",
      "encoder.model.fc.0.bias: Frozen\n",
      "encoder.model.fc.2.weight: Frozen\n",
      "encoder.model.fc.2.bias: Frozen\n",
      "fc1.weight: Trainable\n",
      "fc1.bias: Trainable\n",
      "fc2.weight: Trainable\n",
      "fc2.bias: Trainable\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        self.encoder = base_model  # Use the existing model as the encoder\n",
    "        # Add new layers on top of the encoder\n",
    "        self.fc1 = nn.Linear(base_model.embedding_dim, 64)  # First FC layer\n",
    "        self.relu = nn.ReLU()  # Activation function\n",
    "        self.fc2 = nn.Linear(64, 10)  # Classification head for 10 classes\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax for probabilities\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the encoder\n",
    "        x = self.encoder(x)\n",
    "        # Pass through the new fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)  # Apply softmax for probabilities\n",
    "        return x\n",
    "\n",
    "# Instantiate the custom model\n",
    "custom_model = ClassifierHead(model)\n",
    "\n",
    "# Verify trainable parameters\n",
    "for name, param in custom_model.named_parameters():\n",
    "    print(f\"{name}: {'Trainable' if param.requires_grad else 'Frozen'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Suitable for logits or probabilities\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, custom_model.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters being optimized:\n",
      "Parameter group 0:\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameters being optimized:\")\n",
    "for i, param_group in enumerate(optimizer.param_groups):\n",
    "    print(f\"Parameter group {i}:\")\n",
    "    for param in param_group[\"params\"]:\n",
    "        print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_dir=\"../dataset/data\"\n",
    "mean=[0.4914, 0.4822, 0.4465]\n",
    "std=[0.2470, 0.2435, 0.2616]\n",
    "img_size = 32\n",
    "batch_size = 32\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                transforms.Resize(img_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=mean, std=std)\n",
    "            ])\n",
    "\n",
    "dataset_train = CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
    "subset_indices_train = list(range(1025, 2025))\n",
    "small_dataset_train = torch.utils.data.Subset(dataset_train, subset_indices_train)\n",
    "data_loader_train = DataLoader(small_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_test = CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n",
    "subset_indices_test = list(range(257, 457))\n",
    "small_dataset_test = torch.utils.data.Subset(dataset_test, subset_indices_test)\n",
    "data_loader_test = DataLoader(small_dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training:   0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 68.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 2.2983 | Train Acc: 15.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 166.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Val Loss: 2.2906 | Val Acc: 20.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 129.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Train Loss: 2.2827 | Train Acc: 21.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 170.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Val Loss: 2.2640 | Val Acc: 21.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 130.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Train Loss: 2.2567 | Train Acc: 22.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 170.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Val Loss: 2.2407 | Val Acc: 23.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 129.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Train Loss: 2.2295 | Train Acc: 25.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 170.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Val Loss: 2.2317 | Val Acc: 23.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 129.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Train Loss: 2.2129 | Train Acc: 25.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 170.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Val Loss: 2.2122 | Val Acc: 23.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 129.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Train Loss: 2.1984 | Train Acc: 26.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 170.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Val Loss: 2.2008 | Val Acc: 23.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 129.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Train Loss: 2.1943 | Train Acc: 27.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 170.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Val Loss: 2.1879 | Val Acc: 24.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 129.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Train Loss: 2.1869 | Train Acc: 28.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 171.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Val Loss: 2.1875 | Val Acc: 24.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 130.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Train Loss: 2.1736 | Train Acc: 29.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 169.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Val Loss: 2.1872 | Val Acc: 23.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 129.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Train Loss: 2.1709 | Train Acc: 28.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 169.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Val Loss: 2.1857 | Val Acc: 26.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 129.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Train Loss: 2.1726 | Train Acc: 29.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 170.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Val Loss: 2.1837 | Val Acc: 27.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 129.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Train Loss: 2.1569 | Train Acc: 31.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 171.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Val Loss: 2.1883 | Val Acc: 25.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 130.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Train Loss: 2.1498 | Train Acc: 30.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 170.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Val Loss: 2.1973 | Val Acc: 24.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 130.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Train Loss: 2.1526 | Train Acc: 32.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 171.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Val Loss: 2.1770 | Val Acc: 27.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 130.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Train Loss: 2.1484 | Train Acc: 32.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 170.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Val Loss: 2.1799 | Val Acc: 27.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 129.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Train Loss: 2.1507 | Train Acc: 30.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 169.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Val Loss: 2.1811 | Val Acc: 25.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 129.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Train Loss: 2.1427 | Train Acc: 32.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 170.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Val Loss: 2.1777 | Val Acc: 26.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 130.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Train Loss: 2.1483 | Train Acc: 31.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 170.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Val Loss: 2.1702 | Val Acc: 25.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 130.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Train Loss: 2.1305 | Train Acc: 33.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 170.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Val Loss: 2.1739 | Val Acc: 25.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 129.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Train Loss: 2.1456 | Train Acc: 31.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 170.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Val Loss: 2.1748 | Val Acc: 25.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, custom_model.parameters()), lr=learning_rate)\n",
    "\n",
    "# Move model to device\n",
    "custom_model.to(device)\n",
    "\n",
    "# Training loop with validation\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    custom_model.train()\n",
    "    train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(tqdm(data_loader_train, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Zero the gradient buffers\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = custom_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track metrics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_train += targets.size(0)\n",
    "        correct_train += predicted.eq(targets).sum().item()\n",
    "\n",
    "    train_acc = 100. * correct_train / total_train\n",
    "    avg_train_loss = train_loss / len(data_loader_train)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "    # Validation phase\n",
    "    custom_model.eval()\n",
    "    val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(data_loader_test, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\")):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = custom_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Track metrics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_val += targets.size(0)\n",
    "            correct_val += predicted.eq(targets).sum().item()\n",
    "\n",
    "    val_acc = 100. * correct_val / total_val\n",
    "    avg_val_loss = val_loss / len(data_loader_test)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/new_storage2/Revisiting-Reverse-Distillation/RRDV/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/new_storage2/Revisiting-Reverse-Distillation/RRDV/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters:\n",
      "encoder.conv1.weight: Frozen\n",
      "encoder.bn1.weight: Frozen\n",
      "encoder.bn1.bias: Frozen\n",
      "encoder.layer1.0.conv1.weight: Frozen\n",
      "encoder.layer1.0.bn1.weight: Frozen\n",
      "encoder.layer1.0.bn1.bias: Frozen\n",
      "encoder.layer1.0.conv2.weight: Frozen\n",
      "encoder.layer1.0.bn2.weight: Frozen\n",
      "encoder.layer1.0.bn2.bias: Frozen\n",
      "encoder.layer1.0.conv3.weight: Frozen\n",
      "encoder.layer1.0.bn3.weight: Frozen\n",
      "encoder.layer1.0.bn3.bias: Frozen\n",
      "encoder.layer1.0.downsample.0.weight: Frozen\n",
      "encoder.layer1.0.downsample.1.weight: Frozen\n",
      "encoder.layer1.0.downsample.1.bias: Frozen\n",
      "encoder.layer1.1.conv1.weight: Frozen\n",
      "encoder.layer1.1.bn1.weight: Frozen\n",
      "encoder.layer1.1.bn1.bias: Frozen\n",
      "encoder.layer1.1.conv2.weight: Frozen\n",
      "encoder.layer1.1.bn2.weight: Frozen\n",
      "encoder.layer1.1.bn2.bias: Frozen\n",
      "encoder.layer1.1.conv3.weight: Frozen\n",
      "encoder.layer1.1.bn3.weight: Frozen\n",
      "encoder.layer1.1.bn3.bias: Frozen\n",
      "encoder.layer1.2.conv1.weight: Frozen\n",
      "encoder.layer1.2.bn1.weight: Frozen\n",
      "encoder.layer1.2.bn1.bias: Frozen\n",
      "encoder.layer1.2.conv2.weight: Frozen\n",
      "encoder.layer1.2.bn2.weight: Frozen\n",
      "encoder.layer1.2.bn2.bias: Frozen\n",
      "encoder.layer1.2.conv3.weight: Frozen\n",
      "encoder.layer1.2.bn3.weight: Frozen\n",
      "encoder.layer1.2.bn3.bias: Frozen\n",
      "encoder.layer2.0.conv1.weight: Frozen\n",
      "encoder.layer2.0.bn1.weight: Frozen\n",
      "encoder.layer2.0.bn1.bias: Frozen\n",
      "encoder.layer2.0.conv2.weight: Frozen\n",
      "encoder.layer2.0.bn2.weight: Frozen\n",
      "encoder.layer2.0.bn2.bias: Frozen\n",
      "encoder.layer2.0.conv3.weight: Frozen\n",
      "encoder.layer2.0.bn3.weight: Frozen\n",
      "encoder.layer2.0.bn3.bias: Frozen\n",
      "encoder.layer2.0.downsample.0.weight: Frozen\n",
      "encoder.layer2.0.downsample.1.weight: Frozen\n",
      "encoder.layer2.0.downsample.1.bias: Frozen\n",
      "encoder.layer2.1.conv1.weight: Frozen\n",
      "encoder.layer2.1.bn1.weight: Frozen\n",
      "encoder.layer2.1.bn1.bias: Frozen\n",
      "encoder.layer2.1.conv2.weight: Frozen\n",
      "encoder.layer2.1.bn2.weight: Frozen\n",
      "encoder.layer2.1.bn2.bias: Frozen\n",
      "encoder.layer2.1.conv3.weight: Frozen\n",
      "encoder.layer2.1.bn3.weight: Frozen\n",
      "encoder.layer2.1.bn3.bias: Frozen\n",
      "encoder.layer2.2.conv1.weight: Frozen\n",
      "encoder.layer2.2.bn1.weight: Frozen\n",
      "encoder.layer2.2.bn1.bias: Frozen\n",
      "encoder.layer2.2.conv2.weight: Frozen\n",
      "encoder.layer2.2.bn2.weight: Frozen\n",
      "encoder.layer2.2.bn2.bias: Frozen\n",
      "encoder.layer2.2.conv3.weight: Frozen\n",
      "encoder.layer2.2.bn3.weight: Frozen\n",
      "encoder.layer2.2.bn3.bias: Frozen\n",
      "encoder.layer2.3.conv1.weight: Frozen\n",
      "encoder.layer2.3.bn1.weight: Frozen\n",
      "encoder.layer2.3.bn1.bias: Frozen\n",
      "encoder.layer2.3.conv2.weight: Frozen\n",
      "encoder.layer2.3.bn2.weight: Frozen\n",
      "encoder.layer2.3.bn2.bias: Frozen\n",
      "encoder.layer2.3.conv3.weight: Frozen\n",
      "encoder.layer2.3.bn3.weight: Frozen\n",
      "encoder.layer2.3.bn3.bias: Frozen\n",
      "encoder.layer3.0.conv1.weight: Frozen\n",
      "encoder.layer3.0.bn1.weight: Frozen\n",
      "encoder.layer3.0.bn1.bias: Frozen\n",
      "encoder.layer3.0.conv2.weight: Frozen\n",
      "encoder.layer3.0.bn2.weight: Frozen\n",
      "encoder.layer3.0.bn2.bias: Frozen\n",
      "encoder.layer3.0.conv3.weight: Frozen\n",
      "encoder.layer3.0.bn3.weight: Frozen\n",
      "encoder.layer3.0.bn3.bias: Frozen\n",
      "encoder.layer3.0.downsample.0.weight: Frozen\n",
      "encoder.layer3.0.downsample.1.weight: Frozen\n",
      "encoder.layer3.0.downsample.1.bias: Frozen\n",
      "encoder.layer3.1.conv1.weight: Frozen\n",
      "encoder.layer3.1.bn1.weight: Frozen\n",
      "encoder.layer3.1.bn1.bias: Frozen\n",
      "encoder.layer3.1.conv2.weight: Frozen\n",
      "encoder.layer3.1.bn2.weight: Frozen\n",
      "encoder.layer3.1.bn2.bias: Frozen\n",
      "encoder.layer3.1.conv3.weight: Frozen\n",
      "encoder.layer3.1.bn3.weight: Frozen\n",
      "encoder.layer3.1.bn3.bias: Frozen\n",
      "encoder.layer3.2.conv1.weight: Frozen\n",
      "encoder.layer3.2.bn1.weight: Frozen\n",
      "encoder.layer3.2.bn1.bias: Frozen\n",
      "encoder.layer3.2.conv2.weight: Frozen\n",
      "encoder.layer3.2.bn2.weight: Frozen\n",
      "encoder.layer3.2.bn2.bias: Frozen\n",
      "encoder.layer3.2.conv3.weight: Frozen\n",
      "encoder.layer3.2.bn3.weight: Frozen\n",
      "encoder.layer3.2.bn3.bias: Frozen\n",
      "encoder.layer3.3.conv1.weight: Frozen\n",
      "encoder.layer3.3.bn1.weight: Frozen\n",
      "encoder.layer3.3.bn1.bias: Frozen\n",
      "encoder.layer3.3.conv2.weight: Frozen\n",
      "encoder.layer3.3.bn2.weight: Frozen\n",
      "encoder.layer3.3.bn2.bias: Frozen\n",
      "encoder.layer3.3.conv3.weight: Frozen\n",
      "encoder.layer3.3.bn3.weight: Frozen\n",
      "encoder.layer3.3.bn3.bias: Frozen\n",
      "encoder.layer3.4.conv1.weight: Frozen\n",
      "encoder.layer3.4.bn1.weight: Frozen\n",
      "encoder.layer3.4.bn1.bias: Frozen\n",
      "encoder.layer3.4.conv2.weight: Frozen\n",
      "encoder.layer3.4.bn2.weight: Frozen\n",
      "encoder.layer3.4.bn2.bias: Frozen\n",
      "encoder.layer3.4.conv3.weight: Frozen\n",
      "encoder.layer3.4.bn3.weight: Frozen\n",
      "encoder.layer3.4.bn3.bias: Frozen\n",
      "encoder.layer3.5.conv1.weight: Frozen\n",
      "encoder.layer3.5.bn1.weight: Frozen\n",
      "encoder.layer3.5.bn1.bias: Frozen\n",
      "encoder.layer3.5.conv2.weight: Frozen\n",
      "encoder.layer3.5.bn2.weight: Frozen\n",
      "encoder.layer3.5.bn2.bias: Frozen\n",
      "encoder.layer3.5.conv3.weight: Frozen\n",
      "encoder.layer3.5.bn3.weight: Frozen\n",
      "encoder.layer3.5.bn3.bias: Frozen\n",
      "encoder.layer4.0.conv1.weight: Frozen\n",
      "encoder.layer4.0.bn1.weight: Frozen\n",
      "encoder.layer4.0.bn1.bias: Frozen\n",
      "encoder.layer4.0.conv2.weight: Frozen\n",
      "encoder.layer4.0.bn2.weight: Frozen\n",
      "encoder.layer4.0.bn2.bias: Frozen\n",
      "encoder.layer4.0.conv3.weight: Frozen\n",
      "encoder.layer4.0.bn3.weight: Frozen\n",
      "encoder.layer4.0.bn3.bias: Frozen\n",
      "encoder.layer4.0.downsample.0.weight: Frozen\n",
      "encoder.layer4.0.downsample.1.weight: Frozen\n",
      "encoder.layer4.0.downsample.1.bias: Frozen\n",
      "encoder.layer4.1.conv1.weight: Frozen\n",
      "encoder.layer4.1.bn1.weight: Frozen\n",
      "encoder.layer4.1.bn1.bias: Frozen\n",
      "encoder.layer4.1.conv2.weight: Frozen\n",
      "encoder.layer4.1.bn2.weight: Frozen\n",
      "encoder.layer4.1.bn2.bias: Frozen\n",
      "encoder.layer4.1.conv3.weight: Frozen\n",
      "encoder.layer4.1.bn3.weight: Frozen\n",
      "encoder.layer4.1.bn3.bias: Frozen\n",
      "encoder.layer4.2.conv1.weight: Frozen\n",
      "encoder.layer4.2.bn1.weight: Frozen\n",
      "encoder.layer4.2.bn1.bias: Frozen\n",
      "encoder.layer4.2.conv2.weight: Frozen\n",
      "encoder.layer4.2.bn2.weight: Frozen\n",
      "encoder.layer4.2.bn2.bias: Frozen\n",
      "encoder.layer4.2.conv3.weight: Frozen\n",
      "encoder.layer4.2.bn3.weight: Frozen\n",
      "encoder.layer4.2.bn3.bias: Frozen\n",
      "fc1.weight: Trainable\n",
      "fc1.bias: Trainable\n",
      "fc2.weight: Trainable\n",
      "fc2.bias: Trainable\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "# Define the ClassifierHead\n",
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self, base_model, embedding_dim):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        self.encoder = base_model  # Use the base model as the encoder\n",
    "        self.fc1 = nn.Linear(embedding_dim, 64)  # First FC layer\n",
    "        self.relu = nn.ReLU()  # Activation function\n",
    "        self.fc2 = nn.Linear(64, 10)  # Classification head for 10 classes\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax for probabilities\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the encoder\n",
    "        x = self.encoder(x)\n",
    "        # Pass through the new fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)  # Apply softmax for probabilities\n",
    "        return x\n",
    "\n",
    "# Initialize a randomly initialized ResNet\n",
    "random_resnet = resnet50(pretrained=False)\n",
    "\n",
    "# Replace the final fully connected layer with an identity function\n",
    "random_resnet.fc = nn.Identity()\n",
    "\n",
    "# Freeze all parameters in the ResNet\n",
    "for param in random_resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define the embedding dimension (ResNet-50's output features are 2048)\n",
    "embedding_dim = 2048\n",
    "\n",
    "# Instantiate the custom model\n",
    "custom_model = ClassifierHead(random_resnet, embedding_dim)\n",
    "\n",
    "# Verify trainable parameters\n",
    "print(\"Trainable Parameters:\")\n",
    "for name, param in custom_model.named_parameters():\n",
    "    print(f\"{name}: {'Trainable' if param.requires_grad else 'Frozen'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 120.46it/s, accuracy=10.9, loss=2.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 2.3169, Accuracy: 10.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 156.04it/s, accuracy=7.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 7.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 120.83it/s, accuracy=9.3, loss=2.73] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Loss: 2.3049, Accuracy: 9.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 156.36it/s, accuracy=12.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 12.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 120.29it/s, accuracy=12.3, loss=2.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Loss: 2.2983, Accuracy: 12.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 156.82it/s, accuracy=10.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 10.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 120.94it/s, accuracy=15.1, loss=2.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Loss: 2.2850, Accuracy: 15.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 155.56it/s, accuracy=13.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 13.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 120.62it/s, accuracy=16, loss=2.92]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss: 2.2851, Accuracy: 16.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 156.36it/s, accuracy=10.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 10.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 120.76it/s, accuracy=15.5, loss=2.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Loss: 2.2837, Accuracy: 15.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 155.53it/s, accuracy=10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 10.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 120.60it/s, accuracy=14.1, loss=2.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Loss: 2.2905, Accuracy: 14.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 155.99it/s, accuracy=10.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 10.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 120.57it/s, accuracy=14.7, loss=2.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Loss: 2.2791, Accuracy: 14.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 156.76it/s, accuracy=11.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 11.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 120.60it/s, accuracy=13.6, loss=2.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Loss: 2.2871, Accuracy: 13.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 156.20it/s, accuracy=11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 11.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 120.98it/s, accuracy=15.5, loss=2.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Loss: 2.2817, Accuracy: 15.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 156.22it/s, accuracy=13.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 13.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 120.64it/s, accuracy=14.7, loss=2.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Loss: 2.2821, Accuracy: 14.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 156.64it/s, accuracy=14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 14.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 120.46it/s, accuracy=16, loss=2.91]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Loss: 2.2756, Accuracy: 16.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 156.52it/s, accuracy=14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 14.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 121.11it/s, accuracy=14.7, loss=2.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Loss: 2.2798, Accuracy: 14.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 156.93it/s, accuracy=10.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 10.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 121.13it/s, accuracy=17, loss=2.69]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Loss: 2.2718, Accuracy: 17.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 157.11it/s, accuracy=13.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 13.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 120.75it/s, accuracy=16.5, loss=2.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Loss: 2.2660, Accuracy: 16.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 156.41it/s, accuracy=8.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 8.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 120.82it/s, accuracy=19.4, loss=2.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Loss: 2.2576, Accuracy: 19.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 157.22it/s, accuracy=12.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 12.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 121.01it/s, accuracy=16.2, loss=2.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Loss: 2.2623, Accuracy: 16.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 156.20it/s, accuracy=11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 11.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 120.80it/s, accuracy=17, loss=2.78]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Loss: 2.2578, Accuracy: 17.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 156.20it/s, accuracy=11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 11.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 121.01it/s, accuracy=19, loss=2.68]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Loss: 2.2575, Accuracy: 19.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 156.09it/s, accuracy=7.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 7.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Training: 100%|██████████| 32/32 [00:00<00:00, 120.60it/s, accuracy=20.7, loss=2.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Loss: 2.2392, Accuracy: 20.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Validation: 100%|██████████| 7/7 [00:00<00:00, 157.49it/s, accuracy=14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 14.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "from tqdm import tqdm\n",
    "\n",
    "random_resnet = resnet50(pretrained=False)\n",
    "random_resnet.fc = nn.Identity()  # Replace the final layer with identity\n",
    "for param in random_resnet.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers\n",
    "\n",
    "embedding_dim = 2048\n",
    "custom_model = ClassifierHead(random_resnet, embedding_dim)\n",
    "\n",
    "# Move the model to GPU\n",
    "custom_model = custom_model.cuda()\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, custom_model.parameters()), lr=1e-3)\n",
    "\n",
    "epochs = 20  # Number of epochs\n",
    "for epoch in range(epochs):\n",
    "    custom_model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Wrap the batch iteration with tqdm for progress bar\n",
    "    with tqdm(data_loader_train, desc=f\"Epoch {epoch+1}/{epochs} - Training\") as pbar_train:\n",
    "        for inputs, labels in pbar_train:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()  # Move inputs and labels to GPU\n",
    "\n",
    "            optimizer.zero_grad()  # Clear gradients from previous step\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = custom_model(inputs)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()  # Backpropagate\n",
    "            \n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar_train.set_postfix(loss=running_loss / (pbar_train.n + 1), accuracy=100 * correct / total)\n",
    "\n",
    "    avg_loss = running_loss / len(data_loader_train)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    # Validation loop with tqdm\n",
    "    custom_model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        with tqdm(data_loader_test, desc=f\"Epoch {epoch+1}/{epochs} - Validation\") as pbar_val:\n",
    "            for inputs, labels in pbar_val:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()  # Move inputs and labels to GPU\n",
    "                \n",
    "                outputs = custom_model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                # Update progress bar\n",
    "                pbar_val.set_postfix(accuracy=100 * correct / total)\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RRDV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
